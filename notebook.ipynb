{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Translator\n",
    "\n",
    "Made by <a href=\"https://github.com/SeanvonB\">SeanvonB</a> | <a href=\"https://github.com/SeanvonB/language-translator\">Source</a>\n",
    "\n",
    "This project was part of my [Natural Language Processing Nanodegree](https://www.udacity.com/course/natural-language-processing-nanodegree--nd892), which I completed in late 2020. This particular Nanodegree – in fact, this particular *project* – had been my goal throughout my studies of machine learning. I was just so excited to work on it back then, and I'm still excited to share the work with you now. Machine translation has a long and fascinating history that involved [many](https://en.wikipedia.org/wiki/Rule-based_machine_translation) [different](https://en.wikipedia.org/wiki/Statistical_machine_translation) [approaches](https://en.wikipedia.org/wiki/Example-based_machine_translation) before the widespread commercial adoption of [Neural Machine Translation](https://en.wikipedia.org/wiki/Neural_machine_translation) (NMT) around 2016 or so. The following NMT pipeline, that I created with TensorFlow via Keras, reflects some of the most state-of-the-art theories from that time period, but it was already somewhat outdated when I built it in 2020, thanks largely to [Google Brain](https://research.google/teams/brain/)'s [Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) model with attention.\n",
    "\n",
    "This notebook includes three main sections:\n",
    "1.\tPreprocessing, where I examine, tokenize, and pad the dataset.\n",
    "2.\tModels, where I showcase three different network features on their own before combining them into the final model.\n",
    "3.\tPrediction, where I show how the trained model performs.\n",
    "\n",
    "Let's get started with a whole bunch of workspace helpers and imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ain't nobody got time for training networks on CPU, so this cell simply confirms that the running workspace has access to a GPU, whether through a Udacity Workspace, Amazon Web Services, Google Cloud Platform, or an onboard device. As you can see below, this notebook did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5959198815678897253\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset\n",
    "\n",
    "Language datasets are some of the oldest, largest, and best-maintained datasets available to data science, and the most commonly used translation sets are apparently those from [WMT](http://www.statmt.org/). However, these sets are **enormous**, so Udacity provided truncated versions of these datasets as vocabulary subsets that can train simple networks much faster. These files, for English and French, are located in the `data` directory and will loaded in below using the provided `helper.py` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Sample the Data\n",
    "Each index of `small_vocab_en` and `small_vocab_fr` contain the same sentence in their respective language.\n",
    "\n",
    "The following simply prints the first two pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  paris is sometimes pleasant during october , but it is sometimes quiet in june .\n",
      "small_vocab_fr Line 1:  paris est parfois agréable en octobre , mais il est parfois calme en juin .\n",
      "small_vocab_en Line 2:  new jersey is never hot during june , and it is beautiful in september .\n",
      "small_vocab_fr Line 2:  new jersey est jamais chaud en juin , et il est beau en septembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this data has already undergone some preprocessing, because everything is lowercase and the punctuation is delimited with spaces. This isn't surprising, as these samples come from established datasets that are used for research, but that would otherwise have been Steps 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Vocabulary Complexity\n",
    "\n",
    "In this instance, \"complexity\" refers to the size of the vocabulary and the number of unique words within it. You can probably intuit that more \"complex\" problems require more complex solutions, so the following will provide some insight into the complexity of what Udacity selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1731746 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1862955 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, Lewis Carroll's *Alice's Adventures in Wonderland* has 15,500 total words and 2,766 unique words.\n",
    "\n",
    "So, there isn't that much complexity to this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Tokenize the Vocabulary\n",
    "\n",
    "There are many steps involved in assembling a computer vision pipeline that a natural language processing pipeline can thankfully skip. However, there's one significant difference that must be addressed: unlike image data, language data isn't already numerical. Networks can't perform massive matrix maths on letters.\n",
    "\n",
    "That's where **tokenizing** comes in. Tokenize can occur at the character level; but, for this application, I'll tokenize at the word level. This will create a library of word IDs that each represent one word. Fortunately, this process is very easy with the Keras [`Tokenzier`](https://keras.io/preprocessing/text/#tokenizer) object.\n",
    "\n",
    "I'll also print the outcome as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    \n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "# Test function and print results\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `Tokenizer` simply assigns numbers to words in the order that they appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Pad the Inputs\n",
    "\n",
    "The network will expect every batch of word ID sequences (an abstract way of saying \"batch of sentences\") to be the same length, but that doesn't naturally occur in either dimension: length varies between different sentences within each language and between the same sentence in different languages. Since sentences/sequences are fully dynamic in length, **padding** must be added to the **end** of each sequence to make them all as long as the longest sample in the dataset.\n",
    "\n",
    "Keras provides another function, [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences), for just this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    \n",
    "    return pad_sequences(x, maxlen = length, padding = \"post\")\n",
    "\n",
    "# Test function and print results\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Preprocess Pipeline\n",
    "\n",
    "Here's the full preprocessing pipeline, which includes the above `tokenize` and `pad` functions, plus a `.reshape()` of the data to accomodate how Keras implements the `SparseCategoricalCrossEntropy`, the loss function that I've chosen for this project. Finally, the vocabulary sizes must be increased by `1` to account for the new `<PAD>` token – this dumb thing had me stumped for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 200\n",
      "French vocabulary size: 345\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Loss function requires labels to be in 3D\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "\n",
    "# Add 1 for <PAD> token\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all for data preprocessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Models\n",
    "\n",
    "This section showcases some experimentation with neural network architectures. From the start, I was pretty certain that the final architecture would use all of the tested architecture features; instead, I was mostly just curious how much of an impact each would have on performance.\n",
    "\n",
    "Here are the four architectures that will be shown in this section:\n",
    "1.\tSimple RNN\n",
    "2.\tRNN with Embedding\n",
    "3.\tBidirectional RNN\n",
    "4.\tFinal Model\n",
    "\n",
    "But, first, there's an issue with what all of these models will output..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 IDs to Text\n",
    "\n",
    "Everything that was done to preprocess the data was done to help the network handle it. But, regardless of the architecture, every model must end with a function that converts the base output – a sequence of word IDs – back into sentences that humans can understand. That's what the following `logits_to_text` function does:\n",
    "\n",
    "Note: the word **logit**, in this context, means the highest-probability word ID that the network predicts for a given index within a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model #1: Simple RNN\n",
    "\n",
    "It feels pedantic to say \"a simple RNN\" – y'know, just your run-of-the-mill *Recurrent Neural Network*. There isn't anything simple about RNNs, which I used previously in my [Image Captioner](https://seanvonb.github.io/image-captioner/) project. What RNNs added that prevous neural networks lacked is **memeory between steps**. As you can see in the following diagram, each step passes information both **out of the network** and **forward to the next step**, which allows the network to handle sequential data, like language, where subsequent outputs are determined as much by previous outputs as they are by current inputs.\n",
    "\n",
    "<img src='images/simple.png' width=\"100%\" height=\"auto\" style=\"max-width: 800px;\">\n",
    "\n",
    "But this project will build upon this foundation with some new twists; so, for this notebook, I'll start with a *simple* RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  7/108 [>.............................] - ETA: 42s - loss: 3.5084 - accuracy: 0.3735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    input_seq = Input(input_shape[1:])\n",
    "    rnn = GRU(256, return_sequences = True)(input_seq)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
    "    \n",
    "    model = Model(input_seq, Activation(\"softmax\")(logits))\n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Reshape input to work with base Keras RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's an actual sentence... with essentially the opposite of the intended meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model #2: RNN with Embedding\n",
    "\n",
    "Word IDs are a pretty basic way to represent a word for the network; there's a better way: **word embeddings**. Unlike word IDs, which represent words as a list of integers, word embeddings represent words as vectors in n-dimensional space, i.e. a big cloud of words, where similar words can cluster closer to each other. Word embeddings can help the network understand nuances in language, like how `hot` can be closer to `cold` in one dimension and closer to `sexy` in another. In the example below, you can see word `the` – with the word ID `8` - being embedded as the vector `[0.2, 4, 2.4, 1.1, ...]`, which continues for `n` dimensions.\n",
    "\n",
    "<img src='images/embedding.png' width=\"100%\" height=\"auto\" style=\"max-width: 800px;\">\n",
    "\n",
    "The following uses a Keras `Embedding` layer with `n` set to `256`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 14s 123us/step - loss: 1.3673 - acc: 0.7013 - val_loss: 0.4086 - val_acc: 0.8722\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 13s 120us/step - loss: 0.3158 - acc: 0.8982 - val_loss: 0.2705 - val_acc: 0.9118\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 13s 120us/step - loss: 0.2431 - acc: 0.9190 - val_loss: 0.2315 - val_acc: 0.9227\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 13s 120us/step - loss: 0.2142 - acc: 0.9268 - val_loss: 0.2139 - val_acc: 0.9277\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 13s 120us/step - loss: 0.2022 - acc: 0.9299 - val_loss: 0.2071 - val_acc: 0.9285\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 13s 120us/step - loss: 0.1968 - acc: 0.9313 - val_loss: 0.2080 - val_acc: 0.9292\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 13s 120us/step - loss: 0.1939 - acc: 0.9321 - val_loss: 0.2000 - val_acc: 0.9301\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 13s 120us/step - loss: 0.1929 - acc: 0.9326 - val_loss: 0.2032 - val_acc: 0.9305\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 13s 119us/step - loss: 0.1917 - acc: 0.9328 - val_loss: 0.2039 - val_acc: 0.9299\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 13s 120us/step - loss: 0.1923 - acc: 0.9325 - val_loss: 0.2087 - val_acc: 0.9290\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length = output_sequence_length))\n",
    "    model.add(GRU(256, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = \"softmax\")))\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Reshape input\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "\n",
    "# Train network\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Model #3: Bidirectional RNN\n",
    "\n",
    "An RNN allows the model to handle sequential data, like language; but a bidirectional RNN allows the model to handle language *better*. That's because a bidirectional RNN can also see *future* inputs! That might not be necessary for rote and inflexible sentence structures, but most instances of English will feature split, subordinate, or conditional clauses, phrasal verb tenses, or prepositional phrases – these can cause all manner of unusual splices and inversions of sentence structure. And that's *just* English – I have no idea what linguistic chicanery French gets up to!\n",
    "\n",
    "<img src='images/bidirectional.png' width=\"100%\" height=\"auto\" style=\"max-width: 800px;\">\n",
    "\n",
    "This time, the model features a Keras `Bidirectional` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 18s 165us/step - loss: 2.1304 - acc: 0.5489 - val_loss: 1.4903 - val_acc: 0.6112\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 18s 159us/step - loss: 1.3656 - acc: 0.6257 - val_loss: 1.2708 - val_acc: 0.6424\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 18s 159us/step - loss: 1.2151 - acc: 0.6505 - val_loss: 1.1648 - val_acc: 0.6636\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 1.1233 - acc: 0.6713 - val_loss: 1.0811 - val_acc: 0.6800\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 1.0499 - acc: 0.6846 - val_loss: 1.0170 - val_acc: 0.6919\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 18s 159us/step - loss: 0.9913 - acc: 0.6938 - val_loss: 0.9668 - val_acc: 0.6986\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 18s 159us/step - loss: 0.9465 - acc: 0.7005 - val_loss: 0.9258 - val_acc: 0.7059\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 18s 159us/step - loss: 0.9087 - acc: 0.7067 - val_loss: 0.8895 - val_acc: 0.7108\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 18s 159us/step - loss: 0.8744 - acc: 0.7128 - val_loss: 0.8581 - val_acc: 0.7167\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 18s 159us/step - loss: 0.8452 - acc: 0.7185 - val_loss: 0.8360 - val_acc: 0.7205\n",
      "new jersey est parfois calme en mois et il il il en en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(256, return_sequences = True), input_shape = input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = \"softmax\")))\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model \n",
    "\n",
    "# Train network\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "bd_rnn_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh, that's somehow worse... Oh, of course! Bidirectional must take *twice* as long to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Model #4: Final Model\n",
    "\n",
    "At this point, the architecture is becoming a little complicated, and its training needs are becoming a little less reasonable. But you know I'm still gonna mash the three previous approaches together with some `Dropout` and see what happens. Clearly, the `Embedding` layer had by far the most significant impact; however, I'm curious whether the `Bidirectional` layer will perform better on word embeddings. The following model begs for more training time, but I gave this one the same `10` epochs that the previous models had.\n",
    "\n",
    "Here's the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n",
      "Epoch 1/2\n",
      "103/103 [==============================] - 134s 1s/step - loss: 2.6888 - accuracy: 0.4891 - val_loss: 1.6120 - val_accuracy: 0.6192\n",
      "Epoch 2/2\n",
      "103/103 [==============================] - 131s 1s/step - loss: 1.2411 - accuracy: 0.6909 - val_loss: 0.9025 - val_accuracy: 0.7668\n",
      "1/1 [==============================] - 0s 495ms/step\n",
      "paris est parfois agréable en octobre mais il est parfois calme en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256,\n",
    "                        input_length = output_sequence_length,\n",
    "                        input_shape = input_shape[1:]))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences = True)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = \"softmax\")))\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print('Final Model Loaded')\n",
    "\n",
    "# Train network\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "final_rnn_model = model_final(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "final_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=2, validation_split=0.2)\n",
    "#\n",
    "# # Print prediction(s)\n",
    "print(logits_to_text(final_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost dead on, with only an errant `space` in `l'automne` from the printed sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Prediction\n",
    "\n",
    "This was provided by Udacity to assess my work on the Nanodegree assignment, which was found to be successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "108/108 [==============================] - 161s 1s/step - loss: 2.6241 - accuracy: 0.4959 - val_loss: 1.5304 - val_accuracy: 0.6247\n",
      "Epoch 2/3\n",
      "108/108 [==============================] - 157s 1s/step - loss: 1.1795 - accuracy: 0.7043 - val_loss: 0.8573 - val_accuracy: 0.7729\n",
      "Epoch 3/3\n",
      "108/108 [==============================] - 155s 1s/step - loss: 0.7653 - accuracy: 0.7884 - val_loss: 0.5860 - val_accuracy: 0.8302\n",
      "1/1 [==============================] - 1s 527ms/step\n",
      "Sample 1:\n",
      "il a pas vu camion camion               \n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l' et il il il en en        \n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril       \n"
     ]
    }
   ],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    x = pad(x, max_french_sequence_length)\n",
    "    \n",
    "    model = model_final(\n",
    "        x.shape,\n",
    "        y.shape[1],\n",
    "        english_vocab_size,\n",
    "        french_vocab_size)\n",
    "    model.fit(x, y, batch_size=1024, epochs=3, validation_split=0.2)\n",
    "\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = ''\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still floored by achieving 95% validation accuracy after only 10 epochs, because this model could still benefit from so much more training. Further enhancements to this architecture could also be made, like the encoder-decoder arrangement I used for the [Image Captioner](https://seanvonb.github.io/image-captioner/). I'm so proud to have reached this point, and I hope you found the journey interesting.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by <a href=\"https://github.com/SeanvonB\">SeanvonB</a> | <a href=\"https://github.com/SeanvonB/language-translator\">Source</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
